# 1. Image Generation
> åƒè€ƒè³‡æ–™ï¼š
>- [åŸºæ–¼ Diffusion Models çš„ç”Ÿæˆåœ–åƒæ¼”ç®—æ³•](https://d246810g2000.medium.com/%E5%9F%BA%E6%96%BC-diffusion-models-%E7%9A%84%E7%94%9F%E6%88%90%E5%9C%96%E5%83%8F%E6%BC%94%E7%AE%97%E6%B3%95-984212710610)
>- [AIAIART #7](https://colab.research.google.com/drive/1NFxjNI-UIR7Ku0KERmv7Yb_586vHQW43?usp=sharing#scrollTo=g7btoXL7Im7M)
>- [åŸå§‹è«–æ–‡:Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)

## å¸¸è¦‹åœ–ç‰‡ç”Ÿæˆ
![](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png)
- Autoencoder [2006]
- Variational Autoencoder (VAE) [2014]
- Flow-based models
- GAN [2014]
- PixelRNN [2016]
- Diffusion [2020]

## Defusion Model
Defusion Model æ˜¯æ¨¡ä»¿Markov chainï¼Œåˆ©ç”¨å¸¸æ…‹åˆ†ä½ˆ(é«˜æ–¯åˆ†ä½ˆ)é€æ¼¸å°ä¸€å¼µåœ–ç‰‡å¢åŠ é›œè¨Šï¼Œç›´åˆ°çœ‹ä¸åˆ°åŸå§‹çš„åœ–ç‰‡(ä¸‹åœ–çš„q)ï¼Œå†åˆ©ç”¨æ¨¡å‹ä¸€æ­¥ä¸€æ­¥æŠŠåœ–ç‰‡é‚„åŸå›ä¾†(ä¸‹åœ–çš„p)
![image](https://hackmd.io/_uploads/BJz50T7LR.png)

å¯ä»¥çœ‹ä¸‹å·¦é€™å¼µåœ–é€æ¼¸å¾é›œè¨Šä¸­ç”Ÿå‡ºä¸€å¼µåœ–ã€‚
<div style="display: flex; justify-content: space-around;">
  <div style="text-align: center;">
    <img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*4GmL_x9gzQu3uiH8JKdfaQ.gif" alt="Image 1" style="max-width:97%;">
    <p>ç”Ÿæˆéç¨‹</p>
  </div>
  <div style="text-align: center;">
    <img src="https://truth.bahamut.com.tw/s01/202304/4adfdcb747b978c794f5709339986662.JPG" alt="Image 2" style=";">
    <p>æˆ‘çš„trainå£æ‰çš„ç‰ˆæœ¬</p>
  </div>
</div>



### Diffusion process (æ“´æ•£éç¨‹)
ä¸‹é¢æ˜¯çš„æ„æ€æ˜¯ï¼Œæˆ‘å€‘æœƒé€æ­¥å°ä¸€å¼µåœ–ç‰‡åŠ ä¸Šå¸¸æ…‹åˆ†é…çš„é›œè¨Šï¼Œé€™å€‹æ­¥é©Ÿç¸½å…±æœƒåšTæ¬¡ï¼Œå…¶ä¸­çš„ä¸€æ¬¡å«åštã€‚`Beta_t`æ˜¯ä¸€å€‹æœƒéš¨è‘—tè¶Šå¤§è€Œå¢åŠ çš„å€¼ï¼Œè€Œé›œè¨Šçš„å¸¸æ…‹åˆ†é…çš„å¹³å‡æ•¸(mean)æ˜¯`Beta_t`*ä¸Šä¸€æ­¥é©Ÿt-1çš„åœ–ç‰‡, è®Šç•°æ•¸(var)æ˜¯`Beta_t`ï¼Œå¯ä»¥ç®—å‡ºé›œè¨Šeps, æ¥è‘—æ–°çš„åœ–ç‰‡æœƒæ˜¯`main + âˆšvar*eps`ã€‚

æˆ‘å€‘æ¨¡å‹è¦è¨“ç·´çš„å…¶å¯¦æ˜¯çµ¦å®šä¸€å¼µè¢«é›œè¨Šæ±¡æŸ“çš„`X_t`ï¼Œè¦æ±‚æ¨¡å‹å¹«æˆ‘å€‘é æ¸¬å‡ºæ˜¯ä»€éº¼é›œè¨Šæ±¡æŸ“ä»–ï¼Œä¹Ÿå°±æ˜¯é æ¸¬`eps`

> å…¬å¼

$$
q(\mathbf{x}_{1:T} | \mathbf{x}_0) := \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1}), \quad 
q(\mathbf{x}_t | \mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})
$$

```python=
n_steps = 100
beta = torch.linspace(0.0001, 0.04, n_steps)

def q_xt_xtminus1(xtm1, t):
  # gat
  mean = gather(1. - beta, t) ** 0.5 * xtm1 # âˆš(1âˆ’Î²t)*xtm1
  var = gather(beta, t) # Î²t I
  eps = torch.randn_like(xtm1) # Noise shaped like xtm1
  return mean + (var ** 0.5) * eps


def gather(consts: torch.Tensor, t: torch.Tensor):
    """
    Gather consts for $t$ and reshape to feature map shape
    ç”¨ä¾†æå–ä¸€å€‹tensorä¸­ç¬¬tå€‹å€¼ï¼Œä¸¦éŒ¢reshape
    
    """
    c = consts.gather(-1, t)
    return c.reshape(-1, 1, 1, 1)
```

### Reverse process (é€†æ“´æ•£éç¨‹)
ä¸Šé¢èªªåˆ°æ¨¡å‹éœ€è¦çµ¦å®šåœ–ç‰‡`x_t`ï¼Œé æ¸¬é›œè¨Š`eps`ï¼Œå¯¦éš›ä¸Šæ˜¯è¦è®“æ¨¡å‹å­¸æœƒç”¢ç”Ÿ`eps`çš„å¸¸æ…‹åˆ†é…çš„å¹³å‡å€¼å’Œæ¨™æº–å·®ã€‚

æœ‰äº†ä¸Šé¢çš„`beta_t`ï¼Œå¯ä»¥ç®—å‡º`alpha_t = 1 - beta_t`, `alpha_bar_t = å¾ t=0 é€£ä¹˜åˆ°t=t`ï¼Œæ¥è‘—æŒ‰ä¸‹é¢æ­¥é©Ÿç®—å‡ºï¼š
1. é›œè¨Šçš„ä¿‚æ•¸ eps_coef = `(1-alpha_t )/ âˆš(1-alpha_bar_t)`
2. å¹³å‡mean = `(1/(âˆšalpha_t))*(x_t - eps_coef * noise)`
3. var: å°±æ˜¯`beta_t`
4. æ–°çš„é›œè¨Šeps: ç”¨torchç”Ÿæˆéš¨æ©Ÿæ•¸
5. æ–°çš„åœ–ç‰‡ `main + âˆšvar*eps`
> å…¬å¼ï¼š

$$
p_\theta(\mathbf{x}_{0:T}) := p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t), \quad 
p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) := \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t))
$$

```python=
# Set up some parameters
n_steps = 100
beta = torch.linspace(0.0001, 0.04, n_steps).cuda()
alpha = 1. - beta
alpha_bar = torch.cumprod(alpha, dim=0)

def p_xt(xt, noise, t):
  """
  x_t:è¢«æ±¡æŸ“çš„åœ–ç‰‡
  noiseï¼šæ¨¡å‹çœ‹è‘—x_té æ¸¬å‡ºä¾†çš„é›œè¨Š
  t:ç¾åœ¨æ˜¯ç¬¬å¹¾æ­¥é©Ÿ
  """
  alpha_t = gather(alpha, t)
  alpha_bar_t = gather(alpha_bar, t)
  eps_coef = (1 - alpha_t) / (1 - alpha_bar_t) ** .5
  mean = 1 / (alpha_t ** 0.5) * (xt - eps_coef * noise) # Note minus sign
  var = gather(beta, t)
  eps = torch.randn(xt.shape, device=xt.device)
  return mean + (var ** 0.5) * eps 
```

### Algorithm æ¼”ç®—æ³•
![image](https://hackmd.io/_uploads/HJw4C6Q8C.png)

#### Loss Function
å­¸ç¿’çš„æ™‚å€™æœƒä½¿ç”¨ä¸‹é¢é€™å€‹loss functionä¾†è¨ˆç®—ï¼Œå…¶å¯¦å°±æ˜¯ç®—å‡ºæ±¡æŸ“`x_t`åœ–ç‰‡çš„é›œè¨Š`eps_t`èˆ‡æ¨¡å‹é æ¸¬çš„é›œè¨Š`pred_noise_t`å¹³æ–¹å·®çš„å¹³æ–¹ (mse)
$$
L_{\text{simple}}(\theta) := \mathbb{E}_{t, \mathbf{x}_0, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta \left( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t \right) \right\|^2 \right]
$$

#### Training
é‡è¤‡ä»¥ä¸‹4å€‹æ­¥é©Ÿç›´åˆ°lossé™ä½
1.è¨“ç·´æ™‚å…ˆå¾æˆ‘å€‘æƒ³è¨“ç·´çš„åœ–ç‰‡é›†(`q(x_theata`)ä¸­æŒ‘å‡ºä¸€å¼µ `x_theata`
2.è¨­å®šç¸½è¨“ç·´æ­¥é©Ÿæ•¸å¤§T(ä¹Ÿå°±æ˜¯ä¸Šé¢è¨­å®šçš„`n_step`)ï¼Œä¸­**éš¨æ©Ÿ**æŒ‘æœ±ä¸€å€‹æ­¥é©Ÿåš
3.å…ˆç”¨å¸¸æ…‹åˆ†ä½ˆéš¨æ©Ÿå‡ºä¸€å€‹é›œè¨Š`eps`
4.ç”¨ä¸Šé¢çš„loss functionç®—å‡º`eps`èˆ‡æ¨¡å‹é æ¸¬çš„é›œè¨Š`pred_noise`çš„å¹³æ–¹å·®çš„å¹³æ–¹ï¼Œç„¶å¾Œåšgradient descent

<div style="display: flex-ã„ã„ ; justify-content: space-around;">
  <div style="text-align: left; width: 100%;;">
    <b>Algorithm 1</b> Training
    <pre>
1: repeat
2:   <b>x</b><sub>0</sub> ~ <i>q</i>(<b>x</b><sub>0</sub>)
3:   <i>t</i> ~ Uniform({1, ..., T})
4:   <b>Îµ</b> ~ <i>ğ’©</i>(0, <b>I</b>)
5:   Take gradient descent step on
         âˆ‡<sub>Î¸</sub> || <b>Îµ</b> - <b>Îµ</b><sub>Î¸</sub>(âˆš<span style="text-decoration: overline;">Î±</span><sub>t</sub><b>x</b><sub>0</sub> + âˆš1 - <span style="text-decoration: overline;">Î±</span><sub>t</sub><b>Îµ</b>, <i>t</i>) ||<sup>2</sup>
6: until converged
    </pre>
  </div>
</div>


#### Sampling
è¨“ç·´å¥½ä¹‹å¾Œå°±å¯ä»¥ä¾†ç”¢åœ–äº†ï¼Œç”¢åœ–æµç¨‹å¦‚ä¸‹ï¼š
1. å…ˆç²å¾—ä¸€å€‹é›œè¨Šï¼Œå°±åš`x_T`
2. åŸ·è¡Œå¤§T(n_step)æ¬¡å»å™ªå‹•ä½œï¼Œä»¥ä¸‹ç‚ºloop
    1. ä½¿ç”¨è¨“ç·´å¥½çš„modelç”¢ç”Ÿé æ¸¬çš„é›œè¨Š`pred_eps`
    1. ä½¿ç”¨[Reverse process (é€†æ“´æ•£éç¨‹)](#Reverse-process-é€†æ“´æ•£éç¨‹)çš„å…¬å¼ï¼Œå¹«åœ–ç‰‡é™å™ªï¼Œä¸¦é‚„è¦åŠ ä¸Šä¸€å€‹å°é›œè¨Š
3. loopå®Œä¹‹å¾Œåœ–ç‰‡ç”¢å‡º 

<div style="display: flex-ã„ã„ ; justify-content: space-around;">
  <div style="text-align: left; width: 100%;">
    <b>Algorithm 2</b> Sampling
    <pre>
1: <b>x</b><sub>T</sub> ~ <i>ğ’©</i>(0, <b>I</b>)
2: for <i>t</i> = T, ..., 1 do
3:   <b>z</b> ~ <i>ğ’©</i>(0, <b>I</b>) if <i>t</i> > 1, else <b>z</b> = 0
4:   <b>x</b><sub>t-1</sub> = <sup>1</sup>/<sub>âˆšÎ±<sub>t</sub></sub> (<b>x</b><sub>t</sub> - <sup>1 - Î±<sub>t</sub></sup>/<sub>âˆš1 - <span style="text-decoration: overline;">Î±</span><sub>t</sub></sub> <b>Îµ</b><sub>Î¸</sub>(<b>x</b><sub>t</sub>, <i>t</i>)) + Ïƒ<sub>t</sub><b>z</b>
5: end for
6: return <b>x</b><sub>0</sub>
    </pre>
  </div>
</div>

### æ¨¡å‹ UNet

> åƒè€ƒè³‡æ–™
>- [UNet åŸå§‹è«–æ–‡:U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
>- [ConvTranspose2dåŸç†ï¼Œæ·±åº¦ç½‘ç»œå¦‚ä½•è¿›è¡Œä¸Šé‡‡æ ·ï¼Ÿ](https://blog.csdn.net/qq_27261889/article/details/86304061)

U Net æ˜¯ç”¨ä¾†é æ¸¬é›œè¨Š `pred_eps`çš„æ¨¡å‹æœ¬é«”ï¼Œä»–å¯ä»¥è¼¸å…¥å…©å€‹å€¼ï¼š
1. ç”¨ä¾†é æ¸¬çš„è¢«æ±¡æŸ“çš„åœ–ç‰‡`x_t`
2. ç¾åœ¨æ˜¯ç¬¬å¹¾å€‹æ­¥é©Ÿt

æ­¥é©Ÿæ˜¯
1. down: 2æ¬¡çš„ 3\*3 convolution æ­é…ä¸€æ¬¡2\*2 max poolï¼Œå…±åŸ·è¡Œ4æ¬¡
2. middle(bottom)ï¼Œå…©æ¬¡çš„convolutionï¼Œåœ¨é€™è£¡è¦åŠ æ•¸æ­¥é©Ÿtçš„embeddingè³‡è¨Šï¼Œæ¨¡å‹æ‰æœƒçŸ¥é“ç¾åœ¨åœ¨ç¬¬å¹¾æ­¥é©Ÿ
3. up:2æ¬¡çš„ 3\*3 convolution æ­é…ä¸€æ¬¡2\*2 up-conv(é€†convolution)ï¼Œå…±åŸ·è¡Œ4æ¬¡
![image](https://hackmd.io/_uploads/HJDoCe4LA.png)
