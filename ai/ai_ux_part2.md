# 在ＡＩ技術的成長下，使用者經驗會如何進化 （下）

## Governors 治理者

### Citations（引文功能）

檢索增強生成技術（RAG）的發明，徹底改變了AI回應的複雜程度，以及用戶對其邏輯的透明度。
引文幫助用戶驗證引用的資料是否相關且有效。這樣，用戶不會將內容準確性的控制權完全交給AI的搜尋引擎。

現在，AI不僅僅是總結一個主題或主要來源，還能從多個來源收集資訊並將其整合為單一回應。
引文幫助用戶追溯回應中所包含的資訊至其原始材料。

引文能幫助用戶更深入地了解主題，無論是追蹤內部文件、在研究中發現新的資料，還是驗證某些資訊。
它提供了一個機制來處理資訊匯總這項瑣碎的工作，讓用戶可以專注於編輯與理解。

設計重點：
- 相關的參考資料應該放在引文附近。
- 讓引文具有互動性，可直接讓用戶導到相關資訊
- 允許用戶新增或刪除參考資料，然後重新生成回應和引文，讓用戶擁有完全的控制權。
- 可以考慮在前期加入一些控制，例如過濾條件，確保引用的材料都是特定類型（例如學術文章）或來自特定來源（例如內部知識庫）。


### Controls (控制)

由於用戶請求完成的任務複雜度不同，回應時間可能有所差異。
因此，業界很快產生了一種標準模式，讓用戶能夠控制是否停止或繼續輸出。

最常見的控制是「停止」按鈕，它允許用戶在資料抓取過程中途暫停。
我們可以推測，這背後有商業考量，旨在保留伺服器資源給那些真正想要完成的請求。
從用戶體驗的角度來看，這節省了用戶的時間，讓他們在未得到期望結果時，可以停止並重設他們的提示。

「快轉」按鈕允許用戶在請求超時後繼續進行。
這通常用於AI模型只返回一定數量的資訊，以節省伺服器的處理時間。
例如，當用戶請求100個部落格標題時，模型可能會先返回20個，接下來就由用戶決定是否點擊繼續按鈕來獲取剩下的80個。
這與你在Netflix連續觀看幾小時後收到的「你還在看嗎？」提示有相似之處。

如此的設計賦予用戶對資訊流動的控制權，讓用戶真正感受到他們在引導AI的操作，同時節省用戶時間與減少挫折感。
當用戶未得到預期結果時，這些控制功能可以加入改善提示。例如，當用戶點擊「停止」時，可以顯示提示或建議如何改善請求，甚至引入參數調整。

例如：Adobe Acrobat 將Loading提示結合了清楚的控制按鈕。

<img width="303" alt="截圖 2024-09-26 上午11 49 36" src="https://github.com/user-attachments/assets/6ff3a944-a3f7-4b87-8c52-421836e7e3b5">


### Footprints （足跡）

使用生成式AI有時像是在黑暗中摸索。即使你終於得到了滿意的結果，往往也不知道自己是怎麼達到的。
「足跡」這個概念就是幫助用戶了解AI如何一步步從提示生成出結果，讓用戶有機會追蹤AI的過程，並在需要時介入調整。

- 例子1：Midjourney

當你在Midjourney這樣的圖像生成器中創建一張圖片，然後再進行重生成或修改，這個產品會提供一個鏈接，讓你回到最初的提示。這樣，你可以查看自己一開始輸入的內容，看看是哪些關鍵詞或提示促成了最後的圖像結果。

- 例子2：Adobe Firefly

使用Adobe Firefly等工具創建圖像時，生成的圖片文件會帶有提示的Meta data。這些數據記錄了你輸入的關鍵詞，這樣即使過了很長時間，你也能回頭看一看當時用了什麼詞來生成這個圖像。

![65de12c2603735f271229316_CleanShot 2024-02-27 at 09 40 04@2x](https://github.com/user-attachments/assets/86697c43-5443-41dd-820b-3da817197589)

- 例子3：Github Copilot
在使用Github Copilot寫程式碼時，AI可以自動為生成的代碼加上註解。這樣，如果你不確定為什麼AI會生成這段代碼，你可以點擊註解來查看AI的邏輯，了解它是基於什麼條件或規則來做出這些決定的。
<img width="462" alt="66551d0a32fe33a04b1f4f9a_CleanShot 2024-05-24 at 10 28 41@2x" src="https://github.com/user-attachments/assets/64baba35-ac05-4be3-a938-25ca6c17cf03">

- 例子4：AI文章生成痕跡(無意的足跡):

在社交媒體上，你可能會發現很多內容裡出現類似「作為AI語言模型，我...」這樣的句子。這是因為有些人直接複製AI生成的文字，而沒有進行編輯。這樣的痕跡揭示了這些內容是由AI生成的，而不是人手撰寫的。

- 例子5：Jasper.ai (為什麼足跡很重要):

當你在Jasper.ai中生成一段文字時，你可以讓AI重新生成不同的寫法。但問題是，一旦你接受了新的版本，之前的就消失了。你無法比較兩個不同的版本，甚至無法回顧當初給AI的提示內容。所以，如果你後來想修改AI生成的語氣或風格，或者想查看AI引用了哪些來源，你就無法再追蹤了，這就是為什麼足跡這種模式很重要的原因。

### Prompt transparency(提示透明)

有跡象表明，AI在撰寫提示方面比人類更擅長。
即使在背後運行時，AI生成器會根據你輸入的提示進行邏輯調整，以獲得最佳結果。
這對於自動處理手動工作的AI來說是很好的，但如果你想了解AI是如何達到某些結果的呢？

這時提示的透明性就很重要，不論是明示或暗示，當用戶能理解AI是如何邏輯地生成結果時，他們可以控制偏見、學習如何改進自己的提示能力，並保持對過程的掌控。

#### 暗示

大多數AI產品在處理Input時，不會向用戶展示背後發生的事情。這意味著用戶無法知道AI做了哪些調整。
然而對於生成式AI來說，這可以很容易地解決，你只需要問就可以了。

- 例子1：ChatGPT
當你詢問ChatGPT時，它會告訴你它修改後的提示。這樣，你就可以了解AI如何在背後優化你的提示，從而生成結果。

![66848214812fb09c220408d9_CleanShot 2024-07-01 at 12 31 58@2x](https://github.com/user-attachments/assets/262a1e51-31e3-4fd0-8558-9176f0f68356)

#### 明示

有些生成器會直接顯示提示，將其與生成結果一起顯示，甚至提供AI幫你改進提示的選項。

- 例子2：Udio
在Udio的作品展示中，會顯示用來創作歌曲的提示。這讓用戶能明確看到AI使用了什麼提示來生成結果。

![668494d2a8e7cb1b761bf7f7_CleanShot 2024-07-02 at 18 00 52@2x](https://github.com/user-attachments/assets/98b9e04a-7c78-403b-a831-895b3f9ac805)

提示透明性讓用戶更熟悉AI的操作邏輯，從而提升自己的撰寫提示技術，或者在某些情況下更放心地將責任交給AI來完成任務。


### Regenerate （重新生成）

AI不一定每次都能在第一次給出正確的回應。重新生成提示的功能，能指示AI再次嘗試生成結果。
在許多情況下，這會生成不同的變體，讓用戶可以比較多個選項，選擇最符合需求的結果。

當用戶重新生成時，如果新結果覆蓋了最初的版本，這會形成一種反模式。
在這種情況下，用戶無法結合之前版本中有用的部分與新結果，這會降低用戶的操作靈活度（例如，Github Copilot 的內嵌提示就屬於這種情況）。

用戶可以選擇不斷重新生成回應，或者修改提示、加入調整參數，讓AI朝著更符合他們需求的方向前進。
值得注意的地方是，當用戶第一次生成結果時，可能會有喜歡和不喜歡的部分。如果在重新生成結果時不保留原始結果，可能會讓用戶感到挫折。
應讓用戶保持掌控權，只有在他們明確選擇時才覆蓋舊數據。
使用「足跡」功能保留重新生成的歷史記錄，讓用戶能夠回溯操作過程。

### Sample response （樣本回應）

當我們說到「樣本回應」這個概念，它的目的是確保AI在處理一個複雜的提示之前，先向使用者確認它的理解是否正確。這樣做的好處是避免浪費使用者的時間，並確保生成的結果更符合使用者的需求。

簡單解釋：
假設你在跟AI合作進行一項複雜的任務，像是撰寫一篇文章、生成報告，或是執行一個流程。
AI可能會根據你的初始指令生成一個結果，但是這個結果不一定是你真正想要的。
如果AI在真正完成工作之前，先向你確認它的理解，這樣你就可以避免浪費時間在錯誤的結果上。

具體例子：

1. 撰寫電子郵件： 假設你要AI幫忙撰寫一封商務電子郵件，當你輸入的提示是「請幫我寫一封給客戶的道歉信」，AI可能會猜測你想道歉的原因或信的語氣。但是，在生成完整信件前，AI會展示一個樣本內容，讓你檢查是不是這種風格和方向。如果AI的初步理解錯了，比如語氣太過正式，你可以在它完成之前修正，省去不必要的修改時間。
2. 生成圖片： 假設你使用AI創建一個產品的視覺設計，AI會先根據你的初始描述生成一個草圖或樣本圖片。你可以先查看這個草圖，確認它是否符合你的需求，例如顏色、構圖是否正確。如果不對，你可以立即修改提示，而不是等AI完整生成一個不合格的設計，然後再重新來過。
3. 自動化工作流程： 比如說在一個自動化工具中（像是Zapier），你設置了一個流程來自動處理資料。AI會先使用一條樣本數據來測試流程，並將結果呈現給你，讓你確認流程的每一步是否按預期運行。如果出現錯誤，你可以立刻調整設定，而不需要等到所有資料都處理完再來修改。

這種確認過程可以避免使用者浪費大量時間等待AI生成錯誤的結果，並且讓使用者可以及時調整需求，保證最終結果符合預期。此外，這也讓使用者對AI的操作有更多掌控權，從而增強信任感。


### Show the work (顯示過程)

這個概念是讓AI在給出回答之前，先展示它的思考過程或步驟。這樣使用者就能了解AI是如何得出這個結論的，並且在必要時進行調整或干預。

想像一下你在教學。當你要學生解決一個數學問題時，你希望他們告訴你他們的解題步驟，而不僅僅是給你答案。這樣做不僅能幫助你確保他們理解了問題，也能讓你給予他們更好的指導。

實際例子：
- 例子1 :Julius.ai 和 Perplexity：這些應用會在用戶提出問題時，展示它們獲得答案的過程。例如，當你問「如何做水果沙拉？」AI會展示它的步驟，包括查找食譜、選擇材料等。這樣，使用者可以看到AI的邏輯，並在需要時調整問題或要求更具體的答案。

- 例子2 :GitHub Copilot：當你使用這個工具寫程式碼時，AI會展示它將要編輯的所有檔案。比如說，如果你要求AI添加一個新功能，它會告訴你它會修改哪些檔案和行。這樣，你可以在AI執行操作之前先確認，從而降低錯誤的風險。

- 例子3: Zapier：這個工具會在啟用自動化任務之前，展示它的工作流程。例如，你想自動將電子郵件中的附件儲存到雲端，Zapier會先讓你看到它將如何處理這個任務，並要求你確認。這樣，你可以在操作之前檢查它的邏輯。

  ![665b8c635bcc58b565d652ad_Copilots](https://github.com/user-attachments/assets/714336db-cc17-49e2-8a3a-358917a679d0)

當使用者能看到AI的思考過程時，會更容易信任它的結果。使用者也可以從AI的邏輯中學習，了解如何提出更好的問題或指令。
透過確認AI的步驟，使用者可以提前發現潛在的問題，降低錯誤的風險。
但是對於一些簡單的任務，要求AI先展示過程可能會浪費時間。例如，若用戶只想快速得到一個簡單的回應，讓AI先顯示步驟可能會顯得多此一舉。

### Token transparency (Token透明度)

在AI的運作中，「Token」是信息的基本單位，類似於字詞或符號。當AI生成文字或圖片時，它是根據這些Token來理解和構建內容的。每個Token都有它自己的意義，但使用者可能不知道這些細節。
提供Token透明度的目的是幫助使用者了解AI如何理解他們的輸入。
當使用者知道AI所用的Token時，他們能夠調整自己輸入的提示，使結果更加符合預期。這就像是在一個學習的過程中，使用者可以根據AI的反應不斷改進自己的提問方式。

例如：
假設你使用一個AI圖片生成工具（例如 Midjourney），你輸入的提示是“海灘上的日落”。AI可能會將這個提示分解成數個Token，如「海灘」、「日落」、「天空」等。這些Token會影響生成圖片的每個細節。如果AI能顯示這些Token，使用者就可以更清楚為什麼生成的圖片會有特定的顏色或風格。

在文字生成中，使用者可能會請求AI撰寫一篇關於環境保護的文章。AI可以告訴你它所使用的Token，比如「環境」、「保護」、「可持續性」等。這樣，使用者在看到AI生成的內容後，可以根據這些Token的組合來改進自己的提示，比如加入「氣候變化」或「政策」等Token，以獲得不同的文章視角。

有些Token可能無意中引用了版權作品或帶有偏見。通過查看Token，使用者可以發現這些問題，從而避免在自己的創作中出現不當參考。

### Variations（變體） 

變體指的是AI生成多個結果的不同版本，讓使用者可以選擇最符合需求的一個。這樣的功能可以幫助使用者快速找到理想的結果，尤其在處理複雜的提示時。
變體功能可以主動讓AI從使用者那裡獲取更多信息，讓使用者示範什麼是「理想的結果」，能訓練AI更好地處理使用者的提示。

例如：
假設你正在使用AI生成圖片的工具（例如Midjourney），你請求生成一個「夜晚星空」的圖片。AI會提供多個不同版本的星空圖片，讓你選擇最符合你需求的一個。如果你不喜歡所有的選項，你還可以提供反饋讓AI更好地理解你的需求。

<img width="307" alt="截圖 2024-09-26 下午1 42 57" src="https://github.com/user-attachments/assets/3641f5cf-85cd-42af-ae1f-b719d25ef1f0">

但如果AI不斷顯示多個變體卻無法提供滿意的結果，使用者可能會感到沮喪。為了減少這種情況，AI可以設計一個「這些都不合適」的選項，然後通過進一步的提問來引導使用者提供更多信息。
另外，當變體的選項過多時，使用者可能會出現選擇障礙，從而導致使用者感到困惑，因此，提供合適數量的選項並不斷優化變體是關鍵。


## Trust indicators 信任指標

### Incognito mode （隱私模式）

隱私模式是一個在瀏覽器中非常熟悉的概念。隨著生成式人工智慧（GenAI）不斷重構信息在網絡上的聚合和瀏覽方式，這種功能也開始在AI產品中逐漸流行。

目前，只有ChatGPT實現了私密瀏覽模式，其他基礎模型，如Claude，也開始跟隨ChatGPT的腳步，我們將會看到這項功能慢慢的擴展到其他模型和產品中。
Google則是明確禁止在用戶的瀏覽器隱私模式下使用其AI搜索功能，如欲使用則會回到傳統的搜索界面。從商業角度來看，這是合理的，因為瀏覽器無法從用戶數據中獲益，但卻為AI生成的結果付出高昂的成本。
Meta也有一種小範圍的半隱私模式。雖然用戶無法阻止Meta AI學習他們的偏好，但可以在特定時間內或無限期靜音AI的消息。這樣可以降低該功能的可見性，並讓用戶與之的互動減少。

作為一種新興的模式，目前尚無標準的互動細節。目前最主要的使用方式是遵循瀏覽器隱私模式的模式：
- 允許用戶開始或結束私密會話
- 在隱私模式下，界面樣式略有不同
- 在此模式下，對數據存儲和記憶的差異保持透明
無論用戶是想快速提出請求而不在歷史中留下痕跡，還是希望明確隱藏提示中的某些信息，隱私模式都給予用戶更多控制權，讓他們能夠管理與AI的互動及留下的痕跡。

<img width="307" alt="截圖 2024-09-26 下午2 35 18" src="https://github.com/user-attachments/assets/c7f54e4d-8900-438c-9180-58f9b39b4e27">


### Memory （記憶）

就像我們有時會感到擔憂，搜尋引擎和各種社交平台等，搜集了大量的個人信息，AI在與我們的互動中也會留下相似的數據痕跡。
我們的偏好、興趣和行為，都會逐漸被AI學習和儲存，從而提供更個性化的服務。
然而，這也帶來了關於數據隱私與控制的問題。

AI會存取我們的個人信息並執行任務，這讓AI對我們的了解加深。
這種深入互動對用戶來說有著明顯的好處——AI能預測用戶需求並提供更準確的結果。
然而，用戶會希望能像清除瀏覽器的緩存或調整廣告偏好一樣，有能力管理AI所擁有的個人資料，甚至完全重置這段數據關係。

有時候，可能出於各種原因，用戶希望從頭開始，這時重置選項就顯得至關重要。允許用戶完全清除AI對其的記憶，而不用註銷或重新註冊帳號，能夠有效降低流失率並提高滿意度。
提供這種對記憶的控制不僅增強了用戶的信任感，還能開創新的創造性機會。例如，用戶可以為不同的用途設置不同的記憶集，這樣的靈活性可以激發更多創意。
所有用戶都應享有管理AI記憶的權利。

對AI記憶的控制是一項重要功能，不僅能增強用戶的隱私保護，還能提升AI的使用體驗。當用戶感到自己對AI的數據有掌控權時，他們會對產品有更多的信任，也更願意與之長期互動。

<img width="866" alt="截圖 2024-09-26 下午2 44 23" src="https://github.com/user-attachments/assets/16e77dcb-fac8-4318-84aa-10d8d6186067">


### Watermarks 浮水印

隨著AI技術的普及，識別生成內容（如文本、圖像、音頻和視頻）與人類創建內容之間的區別變得愈加重要。水印（Watermarks）就是為了解決這一需求而設計的，它們可以幫助消費者更自信地篩選網上內容，保護創作者的權益。

許多政府已開始採取措施以強制執行浮水印的使用。中國要求來源生成浮水印，並在下載時在文件名中包含Meta data的足跡模式。
歐盟的AI法案則對標記生成內容提出了類似的標準，並對基礎模型施加透明度和遵守版權法的要求。
在美國，2023年的行政命令建立了浮水印和監管的標準，但需要國會的行動來正式確立這些標準。

浮水印的類型：
1. 疊加浮水印（Overlay Watermarks）：這種浮水印是以視覺符號或文本的形式添加到內容表面，屬於後期處理技術。但這類浮水印相對容易去除，因為它們並未整合到內容的結構中。
2. 隱寫浮水印（Steganographic Watermarks）：這種浮水印會將圖案嵌入內容中，使人類無法察覺。它們可能包括對視頻和圖像像素的細微變化，文本中的額外空格，或音頻文件中的輕微音符變化。雖然這種浮水印的保護程度比疊加浮水印稍高，但仍可能被惡意修改者通過小幅變化（例如加入高斯模糊）掩蓋。
3. 機器學習浮水印（Machine Learning Watermarks）：這是一種新方法，通過機器學習模型為內容添加唯一的特殊密鑰，只有其他模型能讀取。Meta的SynthID就是採用這種方法，這類浮水印的強度高於前兩種，但隨著圖像的修改可能會受到削弱。
4. 統計浮水印（Statistical Watermarks）：這種浮水印是由生成器隨機注入到圖像、音頻或文本中的圖案。雖然它們對內容的影響是表面的（而不是結構性的），但由於其隨機性，破解或遮蓋的難度更大。

Content Provenance（內容來源）：
內容來源是一種將數據嵌入內容本身的替代方法，為內容的Meta data創建數字指紋。這需要內容生成者和數字平台的合作來制定和執行標準。許多大型科技平台已經簽署了一項承諾，將納入由內容來源和真實性聯盟（C2PA）制定的標準，該標準定義了一種無法篡改的通用格式，任何篡改都會留下痕跡。這些數據可以被任何採納該協議的平台讀取，並包括完整的修改歷史。


## Dark matter （暗物質）

### Caveat 警告提示

當我們談到 「警告提示」（caveat） 這個概念時，重點是要提醒使用者生成式AI的局限性，以及它可能會產生的風險或錯誤。隨著AI技術的快速發展，這些模型越來越多地影響我們的日常生活和工作。然而，這些技術並不完美，且時常會產生誤導或不準確的結果。因此，警告提示成為了企業用來保護自己並提醒用戶注意的一種工具。

警告提示主要的作用有：

1. 提示AI的局限性：AI生成的內容並不總是準確的，使用警告提示可以提醒用戶，不應無條件相信AI給出的結果。
2. 減少法律風險：企業通過在產品中加入警告提示，可以將部分責任轉移給用戶。
3. 幫助用戶理解AI的運作方式：警告提示的另一個目的是教育用戶，讓他們了解生成式AI的工作原理及其限制。
4. 提升使用者體驗：例如，可以提供提示告訴用戶如何優化輸入（即如何撰寫更好的提示詞來獲得準確的結果）

超越警告提示更好的方法：

1. 透明化AI運作原理：讓用戶理解AI是如何運作的，並告訴他們AI生成答案的來源或邏輯基礎。
2. 指導用戶創建更好的輸入：提供明確的建議，幫助用戶撰寫更合適的提示詞，以得到更精確的結果。
3. 引入驗證機制：為生成的內容提供來源和引用，讓用戶能夠追溯AI如何得出這些結論。

總結來說，警告提示是生成式AI產品中一個重要的風險管理工具，但企業應該不只做到單純的警告，應尋找更多的方法來幫助用戶理解和有效使用這些技術。隨著技術的不斷演進，使用者對AI的期望也會提高，企業需要確保他們的產品設計考慮到這些挑戰，並提供更完善的用戶體驗。

### Data ownership 數據所有權

在網路平台上個人的數據隱私一直是一個令人擔憂的問題。大型科技公司為了支撐其依賴廣告支持的商業模式，透過「個性化」的名義來合理化對個人數據的存取、重複使用以及轉售。基本上，他們無論如何都會展示廣告和推薦內容，但至少這些中斷行為可以更符合您的興趣，這似乎是給予用戶的一種「好處」。

那當您與AI互動或合作創作內容時，如何界定哪些內容屬於您，哪些屬於平台，還有那些處於兩者之間的灰色地帶？
在這個市場尚未完全確定規則和原則的情況下，許多AI公司已經實施了「數據保留模式」，這種模式現已被廣泛採用。

數據保留模式的標準做法，通常位「設定」中，附上一句簡短的說明，解釋為了改善公司的模型以造福所有用戶，會保留並使用數據，並提供一個開關來讓使用者控制這一功能。

大多數情況下，這個選項預設為「開啟」。例如，ChatGPT、Substack、Github等許多大公司都採取了這樣的做法。
值得注意的是，當Figma在2024年Config大會上宣布其AI功能時，他們選擇將該選項的預設設置為「關閉」。這可能預示著一種以客戶為優先的新趨勢，偏離了傳統的做法。

也有些情況是，選擇退出分享數據的選項僅對付費方案的用戶開放。免費用戶可能根本看不到這個選項，也無法了解到其背後的權衡。
雖然這在商業行為上是合理的，因為運行AI所需的伺服器成本高昂，因此免費用戶以提供其數據作為代價。但隨著有關AI的立法越來越明確，這種方式將面臨來自法規的壓力。

另外，一些公司進一步提升了隱私保障。像Limitless.ai這類公司不會使用用戶數據來訓練模型，並在其隱私政策中明確指出，不允許任何第三方合作夥伴使用其用戶數據進行模型訓練。
如果您採用這種方式，應在設定面板中清楚說明，以免用戶誤以為缺少這類設定意味著缺乏隱私保護。

![66a3c5402c1b4b8fe0e1746a_6681780c0a2a9174abd1970d_gptstealmystuff](https://github.com/user-attachments/assets/14debe8b-84e3-442e-9548-4783f06bc55f)

## Identifiers （識別符號）

### Color scheme 色彩方案

在現代產品設計中，色彩被視為一種強大的辨識工具，特別是在需要幫助用戶快速分辨AI生成內容的場景中。
隨著AI技術滲透到更多日常應用中，顯示哪些內容是來自AI生成變得至關重要。
透過使用特定顏色，如紫色或綠色，產品可以為用戶提供一個明確的視覺提示，告訴他們此部分內容是經由AI生成的，這不僅提升了透明度，也能增強用戶的信任感。

紫色和綠色是目前在AI應用中最常見的顏色選擇。
紫色之所以被廣泛使用，可能是因為它在現代設計中相對常見，且能與日常用戶界面中的藍色形成視覺上的相似性。
同時，綠色則是由於ChatGPT等主流AI平台的品牌選擇，逐漸成為AI技術的代表色之一。
這兩種顏色在視覺上互補，形成了現今大多數AI技術的標誌性配色，且漸變色在這類應用中廣泛使用。

然而僅依賴顏色來作為區分標誌存在一定風險，因為不同用戶的生理與認知能力各異。
比如色盲用戶可能無法有效分辨不同顏色。
因此，單一的色彩信號可能無法滿足所有人的需求，這就要求設計師將顏色與文字或圖標相結合，來確保所有用戶都能理解AI的存在和作用。

<img width="913" alt="截圖 2024-09-27 下午1 23 12" src="https://github.com/user-attachments/assets/00818486-ba20-4109-a609-a5ad9a5a53d2">

### Disclosure 揭露模式

揭露的模式是指標註AI介入的互動和內容，讓用戶能夠區分這些內容與由人類創建的內容，或是非AI介入的互動方式。根據具體情況，可以考慮以下幾種方式：

1. 純AI產品：如果你的產品完全依賴AI技術，如Perplexity或ChatGPT，用戶已經預期會有AI的存在，因此不需要特別強調。但你可以通過區分用戶上傳的內容和AI生成的內容來幫助他們更好理解使用情境。
2. 混合產品：如果你的產品同時包含AI生成或編輯的內容以及人類創建的內容，建議對AI創建的部分進行標註。這樣可以防止用戶誤將AI生成的文本當作自己的創作，並賦予用戶對內容的管理權限。
   例如：IA Writer 7：區分AI創作的文本與人類撰寫的文本。AI生成的文本會顯示為灰色，而人類修改後的文本則會以正常的高對比度顯示。

![664a1471682dd0479402a251_CleanShot 2024-05-19 at 09 01 00@2x](https://github.com/user-attachments/assets/6305241e-d92c-4803-9545-25350744af8c)


3. AI代理：在與聊天機器人互動時，標註訊息是由AI生成的。許多人在與AI聊天時，可能誤以為對方是人類，因此應提前告知。
   例如：Intercom's Fin：明確標註由AI發送的訊息，並且當對話轉接至真人時，會持續在對應訊息上顯示標籤。這樣用戶可以回溯，知道自己從何時開始與真人交談。

![65dfaf63d8e2383d9c8ce2c6_CleanShot 2024-02-28 at 15 10 28@2x](https://github.com/user-attachments/assets/3a500f4e-3f31-4fdd-8e69-8c6bbc4ac7e4)

總結來說，透明揭露AI的存在能夠增加用戶的信任，促進用戶自主權，並且在長遠來看，對產品和品牌都有積極作用。企業應該考慮如何通過透明度來建立競爭優勢，並且避免損害用戶信任的做法。

### Initial CTA 初始號招

當我們討論「Initial CTA」這個概念時，它的核心是在首次與用戶互動時，引導他們體驗產品的AI功能。
許多網站通常以一個大而醒目的輸入框出現，邀請用戶輸入想法或需求，然後由AI根據這些輸入生成相應的內容或建議。

但這種做法面臨了一些挑戰。比如說：

- 第一次使用的用戶因描述不夠具體，無法精確匹配期望，這會導致用戶感到失望。
- 過度簡化的體驗可能變成一種「噱頭」，而非真正讓用戶體驗產品的全部潛力。這會讓用戶覺得這些功能只是「展示性」的，而非解決他們需求的實用工具。

但相對也有一些方式可以提升用戶的體驗，例如：

- 許多產品選擇在用戶進行某些操作後，才引入AI功能。這樣做的好處是，當用戶有明確目標或具體需求時，他們會更容易理解AI能如何幫助他們。
- 在初始CTA中加入趣味性。這不僅可以吸引用戶的興趣，還可以讓他們在輕鬆的情境下理解AI的工作方式。如：Udio允許用戶提出各種有趣或荒誕的創作需求，並能生成有趣的音樂，進而讓人對產品留下深刻的印象。

初始CTA的目的在於讓用戶簡單快速地體驗產品的AI功能，但要謹慎設計這一過程，避免讓用戶對AI的能力產生不切實際的期望。
設計者應考慮提供足夠的引導或範例，來幫助用戶逐步理解並充分發揮AI的潛力。
同時，結合趣味性和引導式的互動可以幫助用戶更好地接受並享受這一過程。

### Name 命名


當我們討論「AI的命名」時，核心問題是：我們該如何稱呼這個AI？是叫它「機器人」？「AI」？還是具體品牌名稱如「ChatGPT」或「Agent」？這個問題不僅關係到用戶體驗，也與信任相關。
命名不僅僅是給AI一個名字，還關係到品牌塑造與用戶信任。清楚標示AI身份，可以幫助用戶在與AI互動時更有信心，同時避免因混淆而產生的信任問題。

以下是幾種常見的AI命名方式：

1. AI作為一個角色或人格 給AI取一個具體的名字，使其更具個性。例如：
  - Intercom：Fin（每次互動時都會顯示AI標誌）
  - Character.ai：每個AI角色都有特定名字，如「問蘇格拉底」
  - Salesforce：Einstein
2. AI與公司名稱相連 直接使用公司的名稱或品牌來命名AI。例如：
  - Jasper：Jasper（問Jasper）
  - ChatGPT：ChatGPT（問ChatGPT）
  - Leena.ai：Leena.ai
3. AI作為一個通用實體 使用通用名詞來描述AI功能，如「助理」或「副駕駛」。
  - Klarna：AI Assistant（問我們的AI助理）
  - Github：Copilot（問Copilot）
  - Zapier：Copilot
4. AI作為技術描述 直接稱為「AI」以強調其技術屬性。
  - Notion：AI（問AI）


目前命名沒有明確的規範，企業可根據其品牌和產品特性選擇合適的名稱。
執得注意的是「恐怖谷效應」： 給AI取一個過於擬人化的名字，可能會讓用戶產生困惑，尤其是當他們無法確定是在與真人還是AI對話時。
但無論選擇哪種名稱，都應使用標誌或其他識別符號，讓用戶清楚知道他們在與AI互動。

![665405b1e0de10a4adbad27b_AiUX Patterns Identifiers](https://github.com/user-attachments/assets/a051676c-54ec-41d2-9cd0-e950dd90fb8a)


### Personality 個性

當我們談論AI的個性時，其實是在探討如何設計和定義AI與用戶互動時所展現出的特徵和風格。這些特徵影響了用戶的體驗，並且能夠增強或削弱用戶對AI的信任感和認同感。

每個大型語言模型（LLM）都會有一個基礎提示，這是模型訓練的基礎，指導著它如何理解和生成語言。這些提示設定了AI的基本特性和行為模式。
AI的個性也可以通過後續的訓練來調整。根據用戶的需求和場景，AI可以被訓練成更具特定風格的助手。

AI的個性應根據用戶的需求調整。例如，在面對困難或敏感的問題時，用戶可能需要更具同理心和理解的互動；而在需要快速和精確的信息時，用戶則希望AI更直接和有效。
當AI能夠展現出一致性和適當的個性時，用戶更有可能信任它。個性化的AI可以讓用戶感受到其需求被重視，從而增加他們的參與度和滿意度。

值得注意的是，如果AI的個性不一致或過於多樣化，可能會導致品牌形象的混亂。用戶可能無法確定他們與哪一個角色互動，進而影響對品牌的信任和認同感。
如果AI的個性過於接近人類，可能會讓用戶感到困惑或不安，尤其是當用戶無法明確區分是與AI還是真實人類交流。這種情況被稱為“恐怖谷效應”（Uncanny Valley）。

AI的個性在設計時需考慮多方面的因素，包括其基礎提示、用戶需求、互動情境等。正確的個性設計能提升用戶體驗和信任感，但不當的設計則可能帶來品牌混淆和用戶不安。因此，在開發AI時，應始終保持用戶的需求和感受為核心，並謹慎設計其個性。

![6684448be8b1b4d6e05f4396_tell me about yourself](https://github.com/user-attachments/assets/21425eb7-156c-481d-b58e-19ae93e8d50a)


### Symbols 代表符號

在數位產品中，符號不僅僅是視覺元素，它們承載了使用者對於AI的理解與期待。當用戶在界面上與AI互動時，符號的設計對於提升用戶體驗和建立信任至關重要。

以下是一些常見的AI符號設計：

- 閃閃發光的星星：這個符號經常用來暗示某種“魔法”或“增強”的效果，是目前最常見的。當用戶看到這個圖標時，他們會聯想到這項功能是由AI驅動的，並且會期望它能提供某種超出常規的體驗。

<img width="800" alt="截圖 2024-09-27 下午2 37 46" src="https://github.com/user-attachments/assets/46f175f5-9950-4635-a19b-5ef8347046bf">

- 發光的圓球：這種符號強調了交互性，尤其在聊天界面中。圓球的設計可能讓用戶感到舒適，因為它給人一種親切感，像是在與一個有智慧的存在進行交流。例如：Siri的球

<img width="316" alt="截圖 2024-09-27 下午2 38 18" src="https://github.com/user-attachments/assets/ace6dde4-0b8f-4ab6-9cd3-e54db77008ee">


- 機器人圖標：機器人的圖像直接表達了自動化和科技的感覺。這種設計讓用戶迅速識別這是AI或自動化工具，而不會將其誤認為人類。

<img width="518" alt="截圖 2024-09-27 下午2 38 06" src="https://github.com/user-attachments/assets/41a9a0ac-3e63-4486-9623-d0e613df24cb">


符號的使用應保持一致，這有助於用戶在不同平台或功能間輕鬆理解AI的角色與功能。例如，如果一個平台使用閃亮星星來表示AI生成的內容，那麼在其他平台上使用相同符號將有助於用戶快速識別。
當多個功能都使用類似的符號時，可能會導致用戶對其功能感到困惑。因此，在符號設計中應避免使用過於模糊或普遍的圖標。

隨著市場上AI產品的增加，能夠有效區分不同品牌或產品的符號設計將變得尤為重要。用戶需要在面對眾多選擇時快速辨識出他們所需的工具。
如果符號設計能夠引發情感共鳴，將會有助於用戶建立對品牌的忠誠。

在AI界面設計中，符號不僅是視覺元素，更是用戶理解和體驗的重要組成部分。設計師應充分考慮符號的選擇與使用，以確保用戶能夠輕鬆識別、理解並信任AI的功能。隨著技術的進步，這些符號的意義和形式可能會演變，因此持續關注用戶反饋和市場趨勢將有助於設計更有效的符號。


# Referances

[Shape of AI](https://www.shapeof.ai)
